{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":4848865,"sourceType":"datasetVersion","datasetId":2810601}],"dockerImageVersionId":30407,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, matthews_corrcoef, roc_auc_score\nimport gc\nimport tensorflow as tf\n\n\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-10-09T16:56:06.462896Z","iopub.execute_input":"2023-10-09T16:56:06.463334Z","iopub.status.idle":"2023-10-09T16:56:18.533106Z","shell.execute_reply.started":"2023-10-09T16:56:06.463290Z","shell.execute_reply":"2023-10-09T16:56:18.530785Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/nfunswnb15v2/NF-UNSW-NB15-V2.parquet\n/kaggle/input/nfunswnb15v2/NetFlow v2 Features.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"features_to_be_removed = ['L4_SRC_PORT', 'L4_DST_PORT', 'Attack', 'Label']\nseed = 42\n\ndef remove_features(df, feats=features_to_be_removed):\n    X = df.drop(columns=feats)\n    y = df.Label\n    return X, y\n\ndef train_test_validation_scaled(X, y, test_size):\n    scaler = MinMaxScaler()\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size,random_state=seed, stratify=y)\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    train_data = X_train[y_train == 0]  # only benign samples from train set\n    attack_data = X_train[y_train == 1]  # only attack samples from train set\n    \n    # Define a validation set\n    idx_for_validation_set = int(train_data.shape[0] * 0.9)\n    val_data = train_data[idx_for_validation_set:]  \n    train_data = train_data[:idx_for_validation_set]\n    \n    del scaler\n    del X_train\n    del y_train\n    gc.collect()\n    \n    return train_data, val_data, X_test, y_test\n\n\ndef load_data(path, test_size=0.2):\n    df = pd.read_parquet(path)\n    X, y = remove_features(df)\n    del df\n    gc.collect()\n    train_data, val_data, X_test, y_test = train_test_validation_scaled(X, y, test_size)\n    return train_data, val_data, X_test, y_test\n\ndef load_model(input_shape):\n    model = tf.keras.models.Sequential([\n                tf.keras.layers.Input(shape=(input_shape,)),\n                tf.keras.layers.Dense(32, activation='relu'),\n                tf.keras.layers.Dense(16, activation='relu'),\n                tf.keras.layers.Dense(8, activation='relu'),\n                tf.keras.layers.Dense(4, activation='relu'),\n                tf.keras.layers.Dense(8, activation='relu'),\n                tf.keras.layers.Dense(16, activation='relu'),\n                tf.keras.layers.Dense(32, activation='relu'),\n                tf.keras.layers.Dense(input_shape, activation='sigmoid')]\n    )\n    \n    return model\n\ndef eval_training(y_test, preds):\n    acc = accuracy_score(y_test, preds)\n    rec = recall_score(y_test, preds)\n    prec = precision_score(y_test, preds)\n    f1 = f1_score(y_test, preds)\n    mcc = matthews_corrcoef(y_test, preds)\n    tn, fp, fn, tp = confusion_matrix(y_test, preds).ravel()\n    missrate = fn / (fn + tp)\n    fallout = fp / (fp + tn)\n    auc = roc_auc_score(y_test, preds)\n    \n    return acc, rec, prec, f1, mcc, missrate, fallout, auc\n\n\ndef test_model_on_dataset(path, supplied_model=None, optimizer='adam', loss='mean_squared_error', batch_size=128, epochs=20):\n    print(f'\\nTESTING DATASET WITH PATH {path}')\n    print('='*80)\n    print()\n        \n    train_data, val_data, X_test, y_test = load_data(path)\n        \n    model = None\n        \n    if supplied_model == None:\n        model = load_model(train_data.shape[1])\n    else:\n        model = supplied_model\n            \n    model.compile(optimizer=optimizer, loss=loss)\n\n    history = model.fit(\n            train_data,\n            train_data,\n            batch_size=batch_size,\n            shuffle=True,\n            epochs=epochs\n    )\n    \n    del train_data \n    gc.collect()\n        \n    val_inference = model.predict(val_data)\n    val_losses = np.mean(abs(val_data - val_inference), axis=1)\n    del val_data\n    gc.collect()\n    \n    threshold = np.quantile(val_losses, 0.95)\n    inference = model.predict(X_test)\n    losses = np.mean(abs(X_test - inference), axis=1)\n    del X_test\n    gc.collect()\n    \n    test_eval = losses > threshold\n    acc, rec, prec, f1, mcc, missrate, fallout, auc = eval_training(y_test, test_eval)\n    \n    print(\"Saving the model\")\n    model.save(\"autoencoder.h5\")\n    print()\n    print(f'ACCURACY: {acc}')\n    print(f'RECALL: {rec}')\n    print(f'PRECISION: {prec}')\n    print(f'F1-SCORE: {f1}')\n    print(f'MATTHEWS CORRELATION COEFFICIENT: {mcc}')\n    print(f'MISSRATE: {missrate}')\n    print(f'FALLOUT: {fallout}')\n    print(f'AUC: {auc}')\n    print()\n        \n        \n\ndef test_model_on_datasets(paths, supplied_model=None, optimizer='adam', loss='mean_squared_error', batch_size=128, epochs=20):\n    \n    for path in paths:\n        \n        test_model_on_dataset(path, supplied_model, optimizer, loss, batch_size, epochs)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:56:32.779410Z","iopub.execute_input":"2023-10-09T16:56:32.780169Z","iopub.status.idle":"2023-10-09T16:56:32.801709Z","shell.execute_reply.started":"2023-10-09T16:56:32.780133Z","shell.execute_reply":"2023-10-09T16:56:32.800689Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"data_paths = ['/kaggle/input/nfunswnb15v2/NF-UNSW-NB15-V2.parquet']\nsampled_data_paths = ['/kaggle/input/sampled-datasets-v2/NF-UNSW-NB15-V2.parquet']\n\ntest_model_on_datasets(data_paths, epochs=10)","metadata":{"execution":{"iopub.status.busy":"2023-10-09T16:56:33.330157Z","iopub.execute_input":"2023-10-09T16:56:33.330646Z","iopub.status.idle":"2023-10-09T17:01:37.882323Z","shell.execute_reply.started":"2023-10-09T16:56:33.330607Z","shell.execute_reply":"2023-10-09T17:01:37.880388Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\nTESTING DATASET WITH PATH /kaggle/input/nfunswnb15v2/NF-UNSW-NB15-V2.parquet\n================================================================================\n\nEpoch 1/10\n10754/10754 [==============================] - 25s 2ms/step - loss: 0.0038\nEpoch 2/10\n10754/10754 [==============================] - 24s 2ms/step - loss: 5.6819e-04\nEpoch 3/10\n10754/10754 [==============================] - 24s 2ms/step - loss: 4.1683e-04\nEpoch 4/10\n10754/10754 [==============================] - 24s 2ms/step - loss: 3.2363e-04\nEpoch 5/10\n10754/10754 [==============================] - 24s 2ms/step - loss: 2.7158e-04\nEpoch 6/10\n10754/10754 [==============================] - 24s 2ms/step - loss: 2.3826e-04\nEpoch 7/10\n10754/10754 [==============================] - 24s 2ms/step - loss: 2.1472e-04\nEpoch 8/10\n10754/10754 [==============================] - 24s 2ms/step - loss: 2.0034e-04\nEpoch 9/10\n10754/10754 [==============================] - 25s 2ms/step - loss: 1.9173e-04\nEpoch 10/10\n10754/10754 [==============================] - 26s 2ms/step - loss: 1.8386e-04\n4780/4780 [==============================] - 7s 1ms/step\n12418/12418 [==============================] - 18s 1ms/step\nSaving the model\n\nACCURACY: 0.9396349305019014\nRECALL: 0.6856686201385189\nPRECISION: 0.3482849604221636\nF1-SCORE: 0.46193189465655704\nMATTHEWS CORRELATION COEFFICIENT: 0.4616328851518688\nMISSRATE: 0.3143313798614811\nFALLOUT: 0.050390628065063704\nAUC: 0.8176389960367276\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}